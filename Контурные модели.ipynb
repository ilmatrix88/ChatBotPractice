{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек\n",
        "!pip install llama-cpp-python fire\n",
        "!pip install zstandard\n",
        "\n",
        "\n",
        "# Скачивание модели\n",
        "!wget https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf\n",
        "\n",
        "# Скачивание датасета\n",
        "!wget https://huggingface.co/datasets/IlyaGusev/ru_turbo_saiga/resolve/main/ru_turbo_saiga.jsonl.zst\n",
        "\n",
        "import json\n",
        "import zstandard as zstd\n",
        "import torch\n",
        "from llama_cpp import Llama\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "\n",
        "\n",
        "# Распаковка датасета\n",
        "with open(\"ru_turbo_saiga.jsonl.zst\", \"rb\") as f:\n",
        "    decompressor = zstd.ZstdDecompressor()\n",
        "    decompressed_data = decompressor.decompress(f.read())\n",
        "    dataset = [json.loads(line) for line in decompressed_data.decode('utf-8').splitlines()]\n",
        "\n",
        "# Загрузка модели\n",
        "model_file = \"model-q4_K.gguf\"\n",
        "model = Llama(model_path=model_file)\n",
        "\n",
        "# Функция оценки образца\n",
        "def evaluate(sample):\n",
        "    # Промпт для образца\n",
        "    prompt = \"\\n\".join([msg[\"content\"] for msg in sample[\"messages\"][:2]])\n",
        "    # Генерируем ответ\n",
        "    outputs = model(prompt, max_tokens=20, temperature=0.2, top_p=0.95, echo=False)\n",
        "    predicted_answer = outputs['choices'][0]['text'].strip()\n",
        "\n",
        "    # Токенизируем истинный и предсказанный ответы\n",
        "    answer = model.tokenize(sample[\"messages\"][2][\"content\"].encode('utf-8'))\n",
        "    predict = model.tokenize(predicted_answer.encode('utf-8'))\n",
        "\n",
        "    # Преобразуем токены в тензоры с плавающей точкой\n",
        "    answer_tensor = torch.tensor(answer, dtype=torch.float32)\n",
        "    predict_tensor = torch.tensor(predict, dtype=torch.float32)\n",
        "\n",
        "    # Выводим типы и формы тензоров для отладки\n",
        "    print(f\"Answer tensor dtype: {answer_tensor.dtype}, shape: {answer_tensor.shape}\")\n",
        "    print(f\"Predict tensor dtype: {predict_tensor.dtype}, shape: {predict_tensor.shape}\")\n",
        "\n",
        "    # Проверяем, что тензоры имеют одинаковую форму\n",
        "    if answer_tensor.shape != predict_tensor.shape:\n",
        "        print(f\"Shapes do not match: {answer_tensor.shape} != {predict_tensor.shape}\")\n",
        "        return 0\n",
        "\n",
        "    # Выравниваем тензоры по длине\n",
        "    min_len = min(answer_tensor.shape[0], predict_tensor.shape[0])\n",
        "    answer_tensor_trimmed = answer_tensor[:min_len]\n",
        "    predict_tensor_trimmed = predict_tensor[:min_len]\n",
        "\n",
        "    # Вычисляем косинусное сходство между истинным и предсказанным ответом\n",
        "    cos_sim = F.cosine_similarity(answer_tensor_trimmed.unsqueeze(0), predict_tensor_trimmed.unsqueeze(0), dim=1)\n",
        "    if cos_sim > 0.5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Массив с оценками\n",
        "success_rate = []\n",
        "number_of_eval_samples = 5\n",
        "\n",
        "# Отбираем записи и делаем по ним цикл с отображением прогресс бара\n",
        "random.shuffle(dataset)\n",
        "for s in tqdm(dataset[:number_of_eval_samples]):\n",
        "    # Результаты оценки складываем в массив\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "# Вычисление точности\n",
        "accuracy = sum(success_rate) / len(success_rate)\n",
        "\n",
        "print(f\"Точность: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysi8naeEoe63",
        "outputId": "e26a76b5-68a5-4ca0-cfc0-7313cbc1d820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "--2024-11-13 20:16:35--  https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.210.66, 13.35.210.114, 13.35.210.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.210.66|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/79/b3/79b3fc4694b2c3a22273003a1de570f145c14f0586c212c28c28e302adf5d3d6/2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1731788195&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTc4ODE5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83OS9iMy83OWIzZmM0Njk0YjJjM2EyMjI3MzAwM2ExZGU1NzBmMTQ1YzE0ZjA1ODZjMjEyYzI4YzI4ZTMwMmFkZjVkM2Q2LzI3OThmMzNmZjYzYzc5MWEyMWYwNWMxZWU5YTEwYmM5NTYzMGIxNzIyNWMxNDBjMTk3MTg4YTNkNWNmMzI2NDQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ensqbwF-q81mFs-Gjj8cEsBPxdFuGWYDb0p3sa0u6mmYgtazPMCYs1tILrd5-pHRDsO0KU5ivEOrHc8zAi37LOhdSF9x0O86r1H4fdr3LdD%7EX2QyhvXel0sG1sGlpvy-PTAFiLmsHRQ1QdH8oFw6L1Wc23nYM2op5ATI3v8u6aJshRwsPuqxTuyx%7EH1p5%7EntEYPHzip4TQXvXmTiWY5Hu3BejRrm%7EV4wPwzimWn68beiXHLNRTvrd-d2e6hnNPBzrEE1A4CGybNIAQ1xymnfqENXb5THaHPrhorfXZ6OYGNpylxCRZo%7EnFUGASGf4DnohhUker%7E2w4u3eCDl%7EakcXA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2024-11-13 20:16:36--  https://cdn-lfs.hf.co/repos/79/b3/79b3fc4694b2c3a22273003a1de570f145c14f0586c212c28c28e302adf5d3d6/2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1731788195&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTc4ODE5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83OS9iMy83OWIzZmM0Njk0YjJjM2EyMjI3MzAwM2ExZGU1NzBmMTQ1YzE0ZjA1ODZjMjEyYzI4YzI4ZTMwMmFkZjVkM2Q2LzI3OThmMzNmZjYzYzc5MWEyMWYwNWMxZWU5YTEwYmM5NTYzMGIxNzIyNWMxNDBjMTk3MTg4YTNkNWNmMzI2NDQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ensqbwF-q81mFs-Gjj8cEsBPxdFuGWYDb0p3sa0u6mmYgtazPMCYs1tILrd5-pHRDsO0KU5ivEOrHc8zAi37LOhdSF9x0O86r1H4fdr3LdD%7EX2QyhvXel0sG1sGlpvy-PTAFiLmsHRQ1QdH8oFw6L1Wc23nYM2op5ATI3v8u6aJshRwsPuqxTuyx%7EH1p5%7EntEYPHzip4TQXvXmTiWY5Hu3BejRrm%7EV4wPwzimWn68beiXHLNRTvrd-d2e6hnNPBzrEE1A4CGybNIAQ1xymnfqENXb5THaHPrhorfXZ6OYGNpylxCRZo%7EnFUGASGf4DnohhUker%7E2w4u3eCDl%7EakcXA__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.155.68.34, 18.155.68.37, 18.155.68.85, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.155.68.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368450336 (4.1G) [binary/octet-stream]\n",
            "Saving to: ‘model-q4_K.gguf.5’\n",
            "\n",
            "model-q4_K.gguf.5   100%[===================>]   4.07G   229MB/s    in 22s     \n",
            "\n",
            "2024-11-13 20:16:58 (191 MB/s) - ‘model-q4_K.gguf.5’ saved [4368450336/4368450336]\n",
            "\n",
            "--2024-11-13 20:16:58--  https://huggingface.co/datasets/IlyaGusev/ru_turbo_saiga/resolve/main/ru_turbo_saiga.jsonl.zst\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.210.66, 13.35.210.114, 13.35.210.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.210.66|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/6d/79/6d798567682b6f4cd426717a2d49b711303a41c438b8e56624c4cfa26aedd107/b2ebca5acdd991e7216cd08b76630b6229df02d1cb1d4d529c226e70be77220f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ru_turbo_saiga.jsonl.zst%3B+filename%3D%22ru_turbo_saiga.jsonl.zst%22%3B&Expires=1731788218&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTc4ODIxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82ZC83OS82ZDc5ODU2NzY4MmI2ZjRjZDQyNjcxN2EyZDQ5YjcxMTMwM2E0MWM0MzhiOGU1NjYyNGM0Y2ZhMjZhZWRkMTA3L2IyZWJjYTVhY2RkOTkxZTcyMTZjZDA4Yjc2NjMwYjYyMjlkZjAyZDFjYjFkNGQ1MjljMjI2ZTcwYmU3NzIyMGY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ehHIbNH7kgnDJEQXaontqA1FnLZGTM6ytRv%7E9bzRW1UDOT4%7ESoKJpSKA3POGSKrthjoK0pFgi2M8rdzadSAWnOsTWDKD6ql3Ay05yBmxLs8qZdR64A6DxhCg0aOtY5vnh6lq%7EO83cmffM8Gd%7EAOMWZRJ-kIuv1xjr-GQgb8zNEp%7EQSobwGIP6B9P9nwTbvO8q5aNgBdM-Gqb4c6W5IPwfxVX-OLzi9E5bGVNkJmmmkQpF2yPne8kjFTXSMndPKjje4eRGkR194hH4J1TQRjTvYbsmm9s-ukF51Q%7EkIIS3v8wQimI-Y240r2GlFc%7E%7EP0OUji%7ExEutUx4Biez6xndnXg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2024-11-13 20:16:58--  https://cdn-lfs.hf.co/repos/6d/79/6d798567682b6f4cd426717a2d49b711303a41c438b8e56624c4cfa26aedd107/b2ebca5acdd991e7216cd08b76630b6229df02d1cb1d4d529c226e70be77220f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ru_turbo_saiga.jsonl.zst%3B+filename%3D%22ru_turbo_saiga.jsonl.zst%22%3B&Expires=1731788218&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTc4ODIxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82ZC83OS82ZDc5ODU2NzY4MmI2ZjRjZDQyNjcxN2EyZDQ5YjcxMTMwM2E0MWM0MzhiOGU1NjYyNGM0Y2ZhMjZhZWRkMTA3L2IyZWJjYTVhY2RkOTkxZTcyMTZjZDA4Yjc2NjMwYjYyMjlkZjAyZDFjYjFkNGQ1MjljMjI2ZTcwYmU3NzIyMGY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ehHIbNH7kgnDJEQXaontqA1FnLZGTM6ytRv%7E9bzRW1UDOT4%7ESoKJpSKA3POGSKrthjoK0pFgi2M8rdzadSAWnOsTWDKD6ql3Ay05yBmxLs8qZdR64A6DxhCg0aOtY5vnh6lq%7EO83cmffM8Gd%7EAOMWZRJ-kIuv1xjr-GQgb8zNEp%7EQSobwGIP6B9P9nwTbvO8q5aNgBdM-Gqb4c6W5IPwfxVX-OLzi9E5bGVNkJmmmkQpF2yPne8kjFTXSMndPKjje4eRGkR194hH4J1TQRjTvYbsmm9s-ukF51Q%7EkIIS3v8wQimI-Y240r2GlFc%7E%7EP0OUji%7ExEutUx4Biez6xndnXg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.155.68.87, 18.155.68.37, 18.155.68.85, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.155.68.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21742388 (21M) [application/octet-stream]\n",
            "Saving to: ‘ru_turbo_saiga.jsonl.zst.5’\n",
            "\n",
            "ru_turbo_saiga.json 100%[===================>]  20.73M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-11-13 20:16:58 (242 MB/s) - ‘ru_turbo_saiga.jsonl.zst.5’ saved [21742388/21742388]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from model-q4_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = models\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: control-looking token: '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 5\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = models\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: EOG token        = 2 '</s>'\n",
            "llm_load_print_meta: EOG token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]llama_perf_context_print:        load time =   33213.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   44980.19 ms /   107 tokens\n",
            " 20%|██        | 1/5 [00:44<02:59, 44.99s/it]Llama.generate: 1 prefix-match hit, remaining 164 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer tensor dtype: torch.float32, shape: torch.Size([16])\n",
            "Predict tensor dtype: torch.float32, shape: torch.Size([19])\n",
            "Shapes do not match: torch.Size([16]) != torch.Size([19])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   33213.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   72850.42 ms /   183 tokens\n",
            " 40%|████      | 2/5 [01:57<03:04, 61.39s/it]Llama.generate: 2 prefix-match hit, remaining 306 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer tensor dtype: torch.float32, shape: torch.Size([29])\n",
            "Predict tensor dtype: torch.float32, shape: torch.Size([19])\n",
            "Shapes do not match: torch.Size([29]) != torch.Size([19])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   33213.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  113791.96 ms /   307 tokens\n",
            " 60%|██████    | 3/5 [03:51<02:50, 85.32s/it]Llama.generate: 2 prefix-match hit, remaining 252 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer tensor dtype: torch.float32, shape: torch.Size([38])\n",
            "Predict tensor dtype: torch.float32, shape: torch.Size([1])\n",
            "Shapes do not match: torch.Size([38]) != torch.Size([1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   33213.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   252 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  106580.56 ms /   271 tokens\n",
            " 80%|████████  | 4/5 [05:38<01:33, 93.72s/it]Llama.generate: 2 prefix-match hit, remaining 127 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer tensor dtype: torch.float32, shape: torch.Size([25])\n",
            "Predict tensor dtype: torch.float32, shape: torch.Size([18])\n",
            "Shapes do not match: torch.Size([25]) != torch.Size([18])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   33213.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   60387.74 ms /   146 tokens\n",
            "100%|██████████| 5/5 [06:38<00:00, 79.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer tensor dtype: torch.float32, shape: torch.Size([27])\n",
            "Predict tensor dtype: torch.float32, shape: torch.Size([19])\n",
            "Shapes do not match: torch.Size([27]) != torch.Size([19])\n",
            "Точность: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек\n",
        "!pip install llama-cpp-python fire\n",
        "!pip install zstandard\n",
        "!pip install nltk\n",
        "\n",
        "import json\n",
        "import zstandard as zstd\n",
        "import torch\n",
        "from llama_cpp import Llama\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Скачивание модели\n",
        "!wget https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf\n",
        "\n",
        "# Скачивание датасета\n",
        "!wget https://huggingface.co/datasets/IlyaGusev/ru_turbo_saiga/resolve/main/ru_turbo_saiga.jsonl.zst\n",
        "\n",
        "# Распаковка датасета\n",
        "with open(\"ru_turbo_saiga.jsonl.zst\", \"rb\") as f:\n",
        "    decompressor = zstd.ZstdDecompressor()\n",
        "    decompressed_data = decompressor.decompress(f.read())\n",
        "    dataset = [json.loads(line) for line in decompressed_data.decode('utf-8').splitlines()]\n",
        "\n",
        "# Загрузка модели\n",
        "model_file = \"model-q4_K.gguf\"\n",
        "model = Llama(model_path=model_file)\n",
        "\n",
        "# Функция оценки образца\n",
        "def evaluate(sample):\n",
        "    # Промпт для образца\n",
        "    prompt = \"\\n\".join([msg[\"content\"] for msg in sample[\"messages\"][:2]])\n",
        "    # Генерируем ответ\n",
        "    outputs = model(prompt, max_tokens=50, temperature=0.2, top_p=0.95, echo=False)  # Уменьшаем количество токенов для генерации\n",
        "    predicted_answer = outputs['choices'][0]['text'].strip()\n",
        "\n",
        "    # Истинный ответ\n",
        "    true_answer = sample[\"messages\"][2][\"content\"]\n",
        "\n",
        "    # Вычисляем BLEU score\n",
        "    bleu_score = sentence_bleu([true_answer.split()], predicted_answer.split())\n",
        "\n",
        "    # Вычисляем BLEU score с использованием функции сглаживания\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    bleu_score = sentence_bleu([true_answer.split()], predicted_answer.split(), smoothing_function=smoothing_function)\n",
        "\n",
        "    # Если BLEU score выше порога, считаем это успехом\n",
        "    if bleu_score > 0.5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Массив с оценками\n",
        "success_rate = []\n",
        "number_of_eval_samples = 3  # Уменьшаем количество образцов для оценки\n",
        "\n",
        "# Отбираем записи и делаем по ним цикл с отображением прогресс бара\n",
        "random.shuffle(dataset)\n",
        "for s in tqdm(dataset[:number_of_eval_samples]):\n",
        "    # Результаты оценки складываем в массив\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "# Вычисление точности\n",
        "accuracy = sum(success_rate) / len(success_rate)\n",
        "\n",
        "print(f\"Точность: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnVvSnuXM4ov",
        "outputId": "9e418972-fb3f-4eba-96fc-929784a25af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "--2024-11-13 20:43:43--  https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.5, 3.169.137.111, 3.169.137.119, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/79/b3/79b3fc4694b2c3a22273003a1de570f145c14f0586c212c28c28e302adf5d3d6/2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1731789823&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTc4OTgyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83OS9iMy83OWIzZmM0Njk0YjJjM2EyMjI3MzAwM2ExZGU1NzBmMTQ1YzE0ZjA1ODZjMjEyYzI4YzI4ZTMwMmFkZjVkM2Q2LzI3OThmMzNmZjYzYzc5MWEyMWYwNWMxZWU5YTEwYmM5NTYzMGIxNzIyNWMxNDBjMTk3MTg4YTNkNWNmMzI2NDQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=T7p2jlSvxDQwcSfk4c32pfq-xopPYkREudanKbksI7qP0bR5wbVb2LkLndP9PuiHod0-0mnRuRhImmRZ8%7EuPJaeBfx0ps12dD5nbFhewefB4Ci%7ED4EzMv1BiO6UQscIrsmcrK0EY52POUMGS9tGqJolljoJ3thEJctCDXBGx3DPsZ67kWqHzFJTDYuDhST9zN6CpLWB2EQUD1ZHIPwIkC69li29HCj6Ax8Oun2j2x%7EypyC4%7E%7E3Z2sr%7EPOfmB3qshQ%7EiFMOs2-%7EuYCYPb5uYPKP44X3UNTSL0D%7Eb4DBzHvkhYLVnOcNNckpGMrsvnqCPkLMhQ7xcsWwBLI0x--4PNWg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2024-11-13 20:43:44--  https://cdn-lfs.hf.co/repos/79/b3/79b3fc4694b2c3a22273003a1de570f145c14f0586c212c28c28e302adf5d3d6/2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1731789823&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTc4OTgyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83OS9iMy83OWIzZmM0Njk0YjJjM2EyMjI3MzAwM2ExZGU1NzBmMTQ1YzE0ZjA1ODZjMjEyYzI4YzI4ZTMwMmFkZjVkM2Q2LzI3OThmMzNmZjYzYzc5MWEyMWYwNWMxZWU5YTEwYmM5NTYzMGIxNzIyNWMxNDBjMTk3MTg4YTNkNWNmMzI2NDQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=T7p2jlSvxDQwcSfk4c32pfq-xopPYkREudanKbksI7qP0bR5wbVb2LkLndP9PuiHod0-0mnRuRhImmRZ8%7EuPJaeBfx0ps12dD5nbFhewefB4Ci%7ED4EzMv1BiO6UQscIrsmcrK0EY52POUMGS9tGqJolljoJ3thEJctCDXBGx3DPsZ67kWqHzFJTDYuDhST9zN6CpLWB2EQUD1ZHIPwIkC69li29HCj6Ax8Oun2j2x%7EypyC4%7E%7E3Z2sr%7EPOfmB3qshQ%7EiFMOs2-%7EuYCYPb5uYPKP44X3UNTSL0D%7Eb4DBzHvkhYLVnOcNNckpGMrsvnqCPkLMhQ7xcsWwBLI0x--4PNWg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.169.121.78, 3.169.121.27, 3.169.121.44, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.169.121.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368450336 (4.1G) [binary/octet-stream]\n",
            "Saving to: ‘model-q4_K.gguf.1’\n",
            "\n",
            "model-q4_K.gguf.1   100%[===================>]   4.07G  39.3MB/s    in 49s     \n",
            "\n",
            "2024-11-13 20:44:33 (84.2 MB/s) - ‘model-q4_K.gguf.1’ saved [4368450336/4368450336]\n",
            "\n",
            "--2024-11-13 20:44:33--  https://huggingface.co/datasets/IlyaGusev/ru_turbo_saiga/resolve/main/ru_turbo_saiga.jsonl.zst\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.111, 3.169.137.19, 3.169.137.5, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/6d/79/6d798567682b6f4cd426717a2d49b711303a41c438b8e56624c4cfa26aedd107/b2ebca5acdd991e7216cd08b76630b6229df02d1cb1d4d529c226e70be77220f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ru_turbo_saiga.jsonl.zst%3B+filename%3D%22ru_turbo_saiga.jsonl.zst%22%3B&Expires=1731789873&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTc4OTg3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82ZC83OS82ZDc5ODU2NzY4MmI2ZjRjZDQyNjcxN2EyZDQ5YjcxMTMwM2E0MWM0MzhiOGU1NjYyNGM0Y2ZhMjZhZWRkMTA3L2IyZWJjYTVhY2RkOTkxZTcyMTZjZDA4Yjc2NjMwYjYyMjlkZjAyZDFjYjFkNGQ1MjljMjI2ZTcwYmU3NzIyMGY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=m8rdZRihVqhkHmdm4KCtTXV-npZg1YP0RCB2RW9Tr6NGrvgggKkvwHCx5vMSmHexwF4yD-E2MEVNF4HIxXUS3P7%7EjFiVV-6Bs5QYt8joiQRjhUEn9fc2jJv86AogOvrrLjgySWcHHz5H5OaDX%7ExXywVdcvKEJ2gS9CLO-6U%7Ewewqdldt7jAJPcYMFLtCCKKsyg4nI%7E3g0QYxe4M6fiOYWmhl0Vb3YFqkcaMwklOYgZ-A7wMZKK-pIqWIbKb5EuFFSSnAut5WRozKUfoXU7fbOLKriK5LI-mypufbdgee6jZRR7p%7EshELYUPVBo8IBn8Nb%7Ew3n5ta8l10aJ0mf1bpPg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2024-11-13 20:44:33--  https://cdn-lfs.hf.co/repos/6d/79/6d798567682b6f4cd426717a2d49b711303a41c438b8e56624c4cfa26aedd107/b2ebca5acdd991e7216cd08b76630b6229df02d1cb1d4d529c226e70be77220f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ru_turbo_saiga.jsonl.zst%3B+filename%3D%22ru_turbo_saiga.jsonl.zst%22%3B&Expires=1731789873&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTc4OTg3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82ZC83OS82ZDc5ODU2NzY4MmI2ZjRjZDQyNjcxN2EyZDQ5YjcxMTMwM2E0MWM0MzhiOGU1NjYyNGM0Y2ZhMjZhZWRkMTA3L2IyZWJjYTVhY2RkOTkxZTcyMTZjZDA4Yjc2NjMwYjYyMjlkZjAyZDFjYjFkNGQ1MjljMjI2ZTcwYmU3NzIyMGY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=m8rdZRihVqhkHmdm4KCtTXV-npZg1YP0RCB2RW9Tr6NGrvgggKkvwHCx5vMSmHexwF4yD-E2MEVNF4HIxXUS3P7%7EjFiVV-6Bs5QYt8joiQRjhUEn9fc2jJv86AogOvrrLjgySWcHHz5H5OaDX%7ExXywVdcvKEJ2gS9CLO-6U%7Ewewqdldt7jAJPcYMFLtCCKKsyg4nI%7E3g0QYxe4M6fiOYWmhl0Vb3YFqkcaMwklOYgZ-A7wMZKK-pIqWIbKb5EuFFSSnAut5WRozKUfoXU7fbOLKriK5LI-mypufbdgee6jZRR7p%7EshELYUPVBo8IBn8Nb%7Ew3n5ta8l10aJ0mf1bpPg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.169.121.27, 3.169.121.44, 3.169.121.78, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.169.121.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21742388 (21M) [application/octet-stream]\n",
            "Saving to: ‘ru_turbo_saiga.jsonl.zst.1’\n",
            "\n",
            "ru_turbo_saiga.json 100%[===================>]  20.73M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-11-13 20:44:34 (197 MB/s) - ‘ru_turbo_saiga.jsonl.zst.1’ saved [21742388/21742388]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from model-q4_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = models\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: control-looking token: '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 5\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = models\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: EOG token        = 2 '</s>'\n",
            "llm_load_print_meta: EOG token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]llama_perf_context_print:        load time =   59118.01 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   95926.11 ms /   175 tokens\n",
            " 33%|███▎      | 1/3 [01:35<03:11, 95.94s/it]Llama.generate: 2 prefix-match hit, remaining 100 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   59118.01 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   82987.74 ms /   149 tokens\n",
            " 67%|██████▋   | 2/3 [02:58<01:28, 88.33s/it]Llama.generate: 2 prefix-match hit, remaining 158 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   59118.01 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   158 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  109509.42 ms /   207 tokens\n",
            "100%|██████████| 3/3 [04:48<00:00, 96.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}